{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pprint\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./generate_csv/train_00.csv',\n",
      " './generate_csv/train_01.csv',\n",
      " './generate_csv/train_02.csv',\n",
      " './generate_csv/train_03.csv',\n",
      " './generate_csv/train_04.csv',\n",
      " './generate_csv/train_05.csv',\n",
      " './generate_csv/train_06.csv',\n",
      " './generate_csv/train_07.csv',\n",
      " './generate_csv/train_08.csv',\n",
      " './generate_csv/train_09.csv',\n",
      " './generate_csv/train_10.csv',\n",
      " './generate_csv/train_11.csv',\n",
      " './generate_csv/train_12.csv',\n",
      " './generate_csv/train_13.csv',\n",
      " './generate_csv/train_14.csv',\n",
      " './generate_csv/train_15.csv',\n",
      " './generate_csv/train_16.csv',\n",
      " './generate_csv/train_17.csv',\n",
      " './generate_csv/train_18.csv',\n",
      " './generate_csv/train_19.csv']\n",
      "['./generate_csv/valid_00.csv',\n",
      " './generate_csv/valid_01.csv',\n",
      " './generate_csv/valid_02.csv',\n",
      " './generate_csv/valid_03.csv',\n",
      " './generate_csv/valid_04.csv',\n",
      " './generate_csv/valid_05.csv',\n",
      " './generate_csv/valid_06.csv',\n",
      " './generate_csv/valid_07.csv',\n",
      " './generate_csv/valid_08.csv',\n",
      " './generate_csv/valid_09.csv']\n",
      "['./generate_csv/test_00.csv',\n",
      " './generate_csv/test_01.csv',\n",
      " './generate_csv/test_02.csv',\n",
      " './generate_csv/test_03.csv',\n",
      " './generate_csv/test_04.csv',\n",
      " './generate_csv/test_05.csv',\n",
      " './generate_csv/test_06.csv',\n",
      " './generate_csv/test_07.csv',\n",
      " './generate_csv/test_08.csv',\n",
      " './generate_csv/test_09.csv']\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"./generate_csv/\"\n",
    "\n",
    "# 通过判断开头去添加文件\n",
    "def get_filename_by_prefix(source_dir, prefix_name):\n",
    "    all_files = os.listdir(source_dir)\n",
    "    results = []\n",
    "    for filename in all_files:\n",
    "        if filename.startswith(prefix_name):\n",
    "            results.append(os.path.join(source_dir, filename))\n",
    "    return results\n",
    "\n",
    "\n",
    "train_filenames = get_filename_by_prefix(source_dir, 'train')\n",
    "valid_filenames = get_filename_by_prefix(source_dir, 'valid')\n",
    "test_filenames = get_filename_by_prefix(source_dir, 'test')\n",
    "pprint.pprint(train_filenames)\n",
    "pprint.pprint(valid_filenames)\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_00.csv',\n",
       " 'test_01.csv',\n",
       " 'test_02.csv',\n",
       " 'test_03.csv',\n",
       " 'test_04.csv',\n",
       " 'test_05.csv',\n",
       " 'test_06.csv',\n",
       " 'test_07.csv',\n",
       " 'test_08.csv',\n",
       " 'test_09.csv',\n",
       " 'train_00.csv',\n",
       " 'train_01.csv',\n",
       " 'train_02.csv',\n",
       " 'train_03.csv',\n",
       " 'train_04.csv',\n",
       " 'train_05.csv',\n",
       " 'train_06.csv',\n",
       " 'train_07.csv',\n",
       " 'train_08.csv',\n",
       " 'train_09.csv',\n",
       " 'train_10.csv',\n",
       " 'train_11.csv',\n",
       " 'train_12.csv',\n",
       " 'train_13.csv',\n",
       " 'train_14.csv',\n",
       " 'train_15.csv',\n",
       " 'train_16.csv',\n",
       " 'train_17.csv',\n",
       " 'train_18.csv',\n",
       " 'train_19.csv',\n",
       " 'valid_00.csv',\n",
       " 'valid_01.csv',\n",
       " 'valid_02.csv',\n",
       " 'valid_03.csv',\n",
       " 'valid_04.csv',\n",
       " 'valid_05.csv',\n",
       " 'valid_06.csv',\n",
       " 'valid_07.csv',\n",
       " 'valid_08.csv',\n",
       " 'valid_09.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_csv_line(line, n_fields=9):\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def csv_reader_dataset(filenames, n_readers=5,\n",
    "                       batch_size=32, n_parse_threads=5,\n",
    "                       shuffile_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()  # 无限次重复服务于epoch\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length=n_readers)\n",
    "    dataset.shuffle(shuffile_buffer_size),\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_set = csv_reader_dataset(train_filenames, batch_size=batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames, batch_size=batch_size)\n",
    "test_set = csv_reader_dataset(test_filenames, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 把train_set,valid_set,test_set存储到tfrecord类型的文件中\n",
    "# 封装\n",
    "def serialize_example(x, y):\n",
    "    \"\"\"Converts x, y to tf.train.Example and serialize\"\"\"\n",
    "    input_features = tf.train.FloatList(value=x)  #特征\n",
    "    label = tf.train.FloatList(value=y)\n",
    "    features = tf.train.Features(\n",
    "        feature={\n",
    "            'input_features': tf.train.Feature(float_list=input_features),\n",
    "            'label': tf.train.Feature(float_list=label)})\n",
    "    example = tf.train.Example(features=features)\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "# n_shards是存为多少个文件，steps_per_shard和 steps_per_epoch类似\n",
    "def csv_dataset_to_tfrecords(base_filename, dataset,\n",
    "                             n_shards, steps_per_shard,\n",
    "                             compression_type=None):\n",
    "    # 压缩文件类型\n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    all_filenames = []\n",
    "    for shard_id in range(n_shards):\n",
    "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(\n",
    "            base_filename, shard_id, n_shards)\n",
    "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
    "            for x_batch, y_batch in dataset.skip(shard_id * steps_per_shard).take(steps_per_shard):\n",
    "                for x_example, y_example in zip(x_batch, y_batch):\n",
    "                    writer.write(serialize_example(x_example, y_example))\n",
    "        all_filenames.append(filename_fullpath)\n",
    "    return all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 49.5 s\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_shards = 20\n",
    "train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "valid_steps_per_shard = 3880 // batch_size // 10\n",
    "test_steps_per_shard = 5170 // batch_size // 10\n",
    "output_dir = \"generate_tfrecords\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir, \"train\")\n",
    "valid_basename = os.path.join(output_dir, \"valid\")\n",
    "test_basename = os.path.join(output_dir, \"test\")\n",
    "\n",
    "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "    train_basename, train_set, n_shards, train_steps_per_shard, None)\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "    valid_basename, valid_set, 10, valid_steps_per_shard, None)\n",
    "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
    "    test_basename, test_set, 10, test_steps_per_shard, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 生成压缩文件\n",
    "# n_shards = 20\n",
    "# train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "# valid_steps_per_shard = 3880 // batch_size // n_shards\n",
    "# test_steps_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "# output_dir = \"generate_tfrecords_zip\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.mkdir(output_dir)\n",
    "\n",
    "# train_basename = os.path.join(output_dir, \"train\")\n",
    "# valid_basename = os.path.join(output_dir, \"valid\")\n",
    "# test_basename = os.path.join(output_dir, \"test\")\n",
    "# #只需修改参数的类型即可\n",
    "# train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "#     train_basename, train_set, n_shards, train_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")\n",
    "# valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "#     valid_basename, valid_set, n_shards, valid_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")\n",
    "# test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
    "#     test_basename, test_set, n_shards, test_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_tfrecords\\\\train_00000-of-00020',\n",
      " 'generate_tfrecords\\\\train_00001-of-00020',\n",
      " 'generate_tfrecords\\\\train_00002-of-00020',\n",
      " 'generate_tfrecords\\\\train_00003-of-00020',\n",
      " 'generate_tfrecords\\\\train_00004-of-00020',\n",
      " 'generate_tfrecords\\\\train_00005-of-00020',\n",
      " 'generate_tfrecords\\\\train_00006-of-00020',\n",
      " 'generate_tfrecords\\\\train_00007-of-00020',\n",
      " 'generate_tfrecords\\\\train_00008-of-00020',\n",
      " 'generate_tfrecords\\\\train_00009-of-00020',\n",
      " 'generate_tfrecords\\\\train_00010-of-00020',\n",
      " 'generate_tfrecords\\\\train_00011-of-00020',\n",
      " 'generate_tfrecords\\\\train_00012-of-00020',\n",
      " 'generate_tfrecords\\\\train_00013-of-00020',\n",
      " 'generate_tfrecords\\\\train_00014-of-00020',\n",
      " 'generate_tfrecords\\\\train_00015-of-00020',\n",
      " 'generate_tfrecords\\\\train_00016-of-00020',\n",
      " 'generate_tfrecords\\\\train_00017-of-00020',\n",
      " 'generate_tfrecords\\\\train_00018-of-00020',\n",
      " 'generate_tfrecords\\\\train_00019-of-00020']\n",
      "['generate_tfrecords\\\\valid_00000-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00001-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00002-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00003-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00004-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00005-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00006-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00007-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00008-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00009-of-00010']\n",
      "['generate_tfrecords\\\\test_00000-of-00010',\n",
      " 'generate_tfrecords\\\\test_00001-of-00010',\n",
      " 'generate_tfrecords\\\\test_00002-of-00010',\n",
      " 'generate_tfrecords\\\\test_00003-of-00010',\n",
      " 'generate_tfrecords\\\\test_00004-of-00010',\n",
      " 'generate_tfrecords\\\\test_00005-of-00010',\n",
      " 'generate_tfrecords\\\\test_00006-of-00010',\n",
      " 'generate_tfrecords\\\\test_00007-of-00010',\n",
      " 'generate_tfrecords\\\\test_00008-of-00010',\n",
      " 'generate_tfrecords\\\\test_00009-of-00010']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_tfrecord_filenames)\n",
    "pprint.pprint(valid_tfrecord_filenames)\n",
    "pprint.pprint(test_tfrecord_fielnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "expected_features = {\n",
    "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
    "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    example = tf.io.parse_single_example(serialized_example,\n",
    "                                         expected_features)\n",
    "    return example[\"input_features\"], example[\"label\"]\n",
    "\n",
    "\n",
    "def tfrecords_reader_dataset(filenames, n_readers=5,\n",
    "                             batch_size=32, n_parse_threads=5,\n",
    "                             shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()  #为了能够无限次epoch\n",
    "    dataset = dataset.interleave(\n",
    "        # lambda filename: tf.data.TFRecordDataset(filename, compression_type = \"GZIP\"),\n",
    "        lambda filename: tf.data.TFRecordDataset(filename),\n",
    "        cycle_length=n_readers)\n",
    "    # 洗牌打乱样本数据顺序\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    # 字节流样本转变为浮点类型\n",
    "    dataset = dataset.map(parse_example,\n",
    "                          num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C575F53A0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C575F53A0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C575F53A0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C575F53A0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function parse_example at 0x0000023C7E1B4310> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function parse_example at 0x0000023C7E1B4310>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function parse_example at 0x0000023C7E1B4310> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function parse_example at 0x0000023C7E1B4310>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "tf.Tensor(\n",
      "[[ 0.70398176  0.5925624  -0.08883677 -0.09038437 -0.35252148 -0.03592975\n",
      "  -0.650416    0.5467585 ]\n",
      " [ 4.8693337  -0.4487407   1.1407809  -0.10629443 -0.07670648 -0.01903702\n",
      "   0.7959478  -1.2858442 ]\n",
      " [ 0.17812423  0.4323619  -0.07598533 -0.13687207 -0.6265219  -0.04002054\n",
      "   1.0758891  -1.2359096 ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.953  ]\n",
      " [5.00001]\n",
      " [1.694  ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.3335598  -1.7303445  -0.00913264  0.1643529   0.53117526 -0.11238974\n",
      "   0.5579976  -0.10239233]\n",
      " [-0.24628098  1.2333642  -0.41765466  0.02003763  0.16009521  0.15687561\n",
      "  -0.7250671   0.6965625 ]\n",
      " [ 0.15419981  1.8741661   0.11231091 -0.05129765 -0.36159435 -0.10304087\n",
      "  -0.846375    0.6965625 ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.119]\n",
      " [1.849]\n",
      " [2.684]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 2.2715902   0.27216142  0.52806157 -0.16666363 -0.5058528  -0.06698878\n",
      "   0.87526447 -1.3607463 ]\n",
      " [-0.7983541   0.19206119 -0.5442215  -0.24129222 -0.35977978  0.1322412\n",
      "  -0.7343984   0.7415038 ]\n",
      " [ 1.031144   -1.890545    0.03064075 -0.0271688  -0.22640869 -0.07940057\n",
      "  -0.92102605  0.88631433]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[3.822]\n",
      " [1.885]\n",
      " [2.4  ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.61308646  0.4323619  -0.37398955  0.08600936 -0.40151492 -0.0138312\n",
      "  -0.72973275  0.7415038 ]\n",
      " [-0.20104307  0.6726626   0.0346156  -0.2670192  -0.4441574  -0.05655285\n",
      "  -0.72040135  0.9562229 ]\n",
      " [-0.04113848 -1.1696428  -0.00764148  0.33453068 -0.17469338 -0.14802444\n",
      "  -0.2118412  -0.61671954]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.356]\n",
      " [1.453]\n",
      " [3.362]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.0135139  -0.92934215 -0.41346925  0.52144337 -0.1275145  -0.00163647\n",
      "  -1.1403134   1.09604   ]\n",
      " [-0.8272339  -0.92934215 -0.44595614 -0.11337186 -0.91140974 -0.21430673\n",
      "  -1.1356478   1.1210073 ]\n",
      " [-0.3755474   0.75276285 -0.53549504 -0.06884047  0.8904606   0.00853772\n",
      "   0.97324395 -1.4506286 ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.063]\n",
      " [0.858]\n",
      " [3.5  ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.47875807  0.75276285 -0.34176412 -0.07935657  0.85054    -0.12206358\n",
      "  -1.3549352   1.2009028 ]\n",
      " [ 2.5424848  -0.6089412   1.0741985  -0.19547    -0.09575949  0.07143254\n",
      "  -0.85104066  0.7614776 ]\n",
      " [ 1.8995097  -0.20843999  1.0692179  -0.12347624  0.06845932 -0.01961886\n",
      "  -0.9163603   0.8164058 ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.894]\n",
      " [3.867]\n",
      " [3.742]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.7472247   0.27216142  0.8062846   0.16529931 -0.5766212  -0.16093802\n",
      "   0.44602105 -1.1859748 ]\n",
      " [-0.91419303  0.99306357 -0.26002774  0.00442114 -0.592045    0.09780166\n",
      "  -0.81838083  0.67658865]\n",
      " [-1.4237458   1.5537652  -0.86738694 -0.22578317  0.14285678  0.06173474\n",
      "  -0.7157357   1.1409812 ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.699]\n",
      " [1.202]\n",
      " [0.708]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.2875226   1.8741661  -0.32764143 -0.18703164 -0.3253029  -0.10202543\n",
      "   1.2065284  -1.5305241 ]\n",
      " [-0.8059737  -0.04823952  0.18792726  0.47425675 -0.84789973 -0.08251984\n",
      "   1.5331267  -0.20226169]\n",
      " [-0.03501084 -1.1696428   0.04031562 -0.08327894 -0.5838795  -0.03315205\n",
      "   1.1458745  -0.856406  ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.523]\n",
      " [0.858]\n",
      " [1.633]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.10939498 -0.92934215  0.02054504 -0.07192681  0.10112162 -0.07670879\n",
      "   1.5611209  -0.7016085 ]\n",
      " [-0.5957692   0.19206119 -0.4831524  -0.03420579  0.6273476   0.01636253\n",
      "  -0.7483955   0.59669316]\n",
      " [-0.05760316  0.51246214  0.002043    0.0051772   0.5484137   0.15150721\n",
      "  -0.78105533  0.7415038 ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.629]\n",
      " [2.542]\n",
      " [1.643]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.9161577   0.5925624   0.1031324  -0.21859623 -0.13840194 -0.00549833\n",
      "  -0.81371516  0.776458  ]\n",
      " [-0.04252386 -1.8104447  -0.39701453 -0.16494337  0.24175097 -0.08885005\n",
      "  -0.9770143   0.9512294 ]\n",
      " [ 0.818755   -0.12833975  0.16940758 -0.02182356  0.13015477 -0.02852592\n",
      "  -0.8417093   0.60168666]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.052]\n",
      " [2.223]\n",
      " [3.79 ]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 测试tfrecords_reader_dataset\n",
    "tfrecords_train = tfrecords_reader_dataset(train_tfrecord_filenames,\n",
    "                                           batch_size=3)\n",
    "for x_batch, y_batch in tfrecords_train.take(10):\n",
    "    print(x_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C7DFC60D0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C7DFC60D0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C7DFC60D0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C7DFC60D0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C575F5430> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C575F5430>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C575F5430> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C575F5430>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C7E11C670> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C7E11C670>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C7E11C670> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x0000023C7E11C670>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[ 0.70398176,  0.5925624 , -0.08883677, -0.09038437, -0.35252148,\n",
      "        -0.03592975, -0.650416  ,  0.5467585 ],\n",
      "       [-0.24628098,  1.2333642 , -0.41765466,  0.02003763,  0.16009521,\n",
      "         0.15687561, -0.7250671 ,  0.6965625 ],\n",
      "       [-0.25134295,  0.11196095, -0.4245842 , -0.17900464, -0.07035547,\n",
      "         0.09978117, -0.8790348 ,  0.7914384 ],\n",
      "       [-0.51040864,  0.5925624 ,  0.1138471 , -0.06411297, -0.5149257 ,\n",
      "        -0.02267715,  1.0712235 , -1.0611382 ],\n",
      "       [-0.09340986,  0.51246214, -0.09711546, -0.3229636 , -0.74719095,\n",
      "         0.07175772, -0.8090495 ,  0.62665397],\n",
      "       [ 0.15419981,  1.8741661 ,  0.11231091, -0.05129765, -0.36159435,\n",
      "        -0.10304087, -0.846375  ,  0.6965625 ],\n",
      "       [-0.61308646,  0.4323619 , -0.37398955,  0.08600936, -0.40151492,\n",
      "        -0.0138312 , -0.72973275,  0.7415038 ],\n",
      "       [-0.86122894,  0.5925624 , -0.89267266,  0.04866396,  1.0102223 ,\n",
      "        -0.03401763, -0.7064043 ,  0.6616083 ],\n",
      "       [-0.75791174, -1.7303445 , -0.34140635, -0.03207467, -0.32983932,\n",
      "        -0.06002742,  1.6404376 , -1.0261838 ],\n",
      "       [ 0.05823575,  1.8741661 ,  0.26091635, -0.25543734, -0.8016281 ,\n",
      "        -0.0912847 ,  0.9779097 , -1.2858442 ],\n",
      "       [-0.20104307,  0.6726626 ,  0.0346156 , -0.2670192 , -0.4441574 ,\n",
      "        -0.05655285, -0.72040135,  0.9562229 ],\n",
      "       [-0.3755474 ,  0.75276285, -0.53549504, -0.06884047,  0.8904606 ,\n",
      "         0.00853772,  0.97324395, -1.4506286 ],\n",
      "       [-0.06165273,  0.51246214,  0.18503915, -0.01769356,  0.09295604,\n",
      "         0.10990728, -0.6550817 ,  0.56673235],\n",
      "       [-0.82094646, -0.4487407 , -0.45438793, -0.24141093, -0.10029592,\n",
      "         0.05197262, -1.4295862 ,  1.2358571 ],\n",
      "       [-0.33883488,  0.35226166, -0.48561847, -0.16123001,  0.0367043 ,\n",
      "        -0.09798459,  0.80061346, -1.2059487 ],\n",
      "       [-0.47875807,  0.75276285, -0.34176412, -0.07935657,  0.85054   ,\n",
      "        -0.12206358, -1.3549352 ,  1.2009028 ],\n",
      "       [-0.91419303,  0.99306357, -0.26002774,  0.00442114, -0.592045  ,\n",
      "         0.09780166, -0.81838083,  0.67658865],\n",
      "       [-0.332281  , -1.0094423 , -0.22360498, -0.15263619, -0.22822326,\n",
      "        -0.0263794 ,  1.7710769 , -2.104773  ],\n",
      "       [-0.6699935 ,  0.6726626 , -0.7394774 , -0.391156  , -0.06763361,\n",
      "         0.2195694 , -0.78105533,  0.6666017 ],\n",
      "       [ 0.34389004,  1.3935647 ,  0.14046204, -0.14520583, -0.83973414,\n",
      "        -0.06430953, -0.7997181 ,  0.60168666],\n",
      "       [-1.4237458 ,  1.5537652 , -0.86738694, -0.22578317,  0.14285678,\n",
      "         0.06173474, -0.7157357 ,  1.1409812 ],\n",
      "       [-0.10939498, -0.92934215,  0.02054504, -0.07192681,  0.10112162,\n",
      "        -0.07670879,  1.5611209 , -0.7016085 ],\n",
      "       [-0.8719923 , -0.52884096, -0.76052576, -0.12795003, -0.21733584,\n",
      "         0.09166765, -0.7623926 ,  0.73151684],\n",
      "       [-0.16278532, -0.92934215,  0.3292493 ,  0.15335903,  0.4449831 ,\n",
      "        -0.13386014, -0.23516965, -0.56179136],\n",
      "       [ 0.6988665 ,  0.5925624 ,  0.14489786, -0.03596503, -0.3116936 ,\n",
      "        -0.06197797, -1.2336272 ,  1.1509681 ],\n",
      "       [-0.5957692 ,  0.19206119, -0.4831524 , -0.03420579,  0.6273476 ,\n",
      "         0.01636253, -0.7483955 ,  0.59669316],\n",
      "       [ 0.818755  , -0.12833975,  0.16940758, -0.02182356,  0.13015477,\n",
      "        -0.02852592, -0.8417093 ,  0.60168666],\n",
      "       [ 0.26902634, -0.7691417 , -0.2894629 , -0.36774153, -0.9268336 ,\n",
      "        -0.1112351 ,  0.9405841 , -1.2758573 ],\n",
      "       [-0.710649  ,  0.11196095, -0.5937465 , -0.04276745,  0.63279134,\n",
      "        -0.05181003, -1.401592  ,  1.2408506 ],\n",
      "       [-0.83948916, -0.36864048, -0.67464167, -0.02348134,  0.90044075,\n",
      "        -0.1348634 , -0.6877415 ,  0.5867062 ],\n",
      "       [-0.84407157,  1.8741661 , -0.17332736, -0.23671   , -0.4114951 ,\n",
      "        -0.10528539,  2.400945  , -2.2895312 ],\n",
      "       [-0.586711  , -1.7303445 ,  0.45947346,  0.20239025, -0.85606533,\n",
      "         0.22319904, -0.49178258,  0.8263927 ]], dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[1.953],\n",
      "       [1.849],\n",
      "       [1.909],\n",
      "       [1.311],\n",
      "       [1.845],\n",
      "       [2.684],\n",
      "       [2.356],\n",
      "       [2.559],\n",
      "       [1.038],\n",
      "       [2.19 ],\n",
      "       [1.453],\n",
      "       [3.5  ],\n",
      "       [1.782],\n",
      "       [1.208],\n",
      "       [2.618],\n",
      "       [2.894],\n",
      "       [1.202],\n",
      "       [1.162],\n",
      "       [1.125],\n",
      "       [2.463],\n",
      "       [0.708],\n",
      "       [1.629],\n",
      "       [1.625],\n",
      "       [3.578],\n",
      "       [3.878],\n",
      "       [2.542],\n",
      "       [3.79 ],\n",
      "       [1.63 ],\n",
      "       [1.427],\n",
      "       [2.692],\n",
      "       [0.927],\n",
      "       [1.256]], dtype=float32)>)\n",
      "CPU times: total: 109 ms\n",
      "Wall time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#dataset的tensor，可以直接拿tensor训练\n",
    "batch_size = 32\n",
    "tfrecords_train_set = tfrecords_reader_dataset(\n",
    "    train_tfrecord_filenames, batch_size=batch_size)\n",
    "tfrecords_valid_set = tfrecords_reader_dataset(\n",
    "    valid_tfrecord_filenames, batch_size=batch_size)\n",
    "tfrecords_test_set = tfrecords_reader_dataset(\n",
    "    test_tfrecord_fielnames, batch_size=batch_size)\n",
    "print(type(tfrecords_train_set))\n",
    "for i in tfrecords_train_set.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.9300 - val_loss: 0.5469\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.5003 - val_loss: 0.4957\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4529 - val_loss: 0.4637\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4413 - val_loss: 0.4475\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.4241 - val_loss: 0.4339\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.4210\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4039 - val_loss: 0.4222\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.3939 - val_loss: 0.3996\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.3966\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.4471 - val_loss: 0.4058\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3928 - val_loss: 0.3937\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.3927\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3766 - val_loss: 0.3819\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3783 - val_loss: 0.3827\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4287 - val_loss: 0.4361\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 1.3443\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.3798\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3875 - val_loss: 0.3756\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu',\n",
    "                       input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    patience=5, min_delta=1e-2)]\n",
    "history = model.fit(tfrecords_train_set,\n",
    "                    validation_data=tfrecords_valid_set,\n",
    "                    steps_per_epoch=11160 // batch_size,\n",
    "                    validation_steps=3870 // batch_size,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 829us/step - loss: 0.3699\n",
      "0.3699362576007843\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(tfrecords_test_set, steps=5160 // batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
