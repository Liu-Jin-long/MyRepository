{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b157efb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-30T11:56:23.675255Z",
     "iopub.status.busy": "2022-07-30T11:56:23.674486Z",
     "iopub.status.idle": "2022-07-30T11:57:08.848397Z",
     "shell.execute_reply": "2022-07-30T11:57:08.847287Z"
    },
    "papermill": {
     "duration": 45.182834,
     "end_time": "2022-07-30T11:57:08.850818",
     "exception": false,
     "start_time": "2022-07-30T11:56:23.667984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.13.1\r\n",
      "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\r\n",
      "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.6/367.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.1.0)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.43.0)\r\n",
      "Collecting keras-applications>=1.0.6\r\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.16.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.37.1)\r\n",
      "Collecting astor>=0.6.0\r\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\r\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.4.0)\r\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (3.19.4)\r\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.1.0)\r\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\r\n",
      "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.1.2)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.21.6)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.7.0)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.1.2)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.7)\r\n",
      "Requirement already satisfied: mock>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.12.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.8.0)\r\n",
      "Installing collected packages: tensorflow-estimator, astor, keras-applications, tensorboard, tensorflow\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\r\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.6.0\r\n",
      "    Uninstalling tensorboard-2.6.0:\r\n",
      "      Successfully uninstalled tensorboard-2.6.0\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.6.4\r\n",
      "    Uninstalling tensorflow-2.6.4:\r\n",
      "      Successfully uninstalled tensorflow-2.6.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\r\n",
      "tfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 1.13.1 which is incompatible.\r\n",
      "tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 1.13.1 which is incompatible.\r\n",
      "tensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 1.13.1 which is incompatible.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 1.13.1 which is incompatible.\r\n",
      "tensorflow-cloud 0.1.14 requires tensorboard>=2.3.0, but you have tensorboard 1.13.1 which is incompatible.\r\n",
      "tensorflow-cloud 0.1.14 requires tensorflow<3.0,>=1.15.0, but you have tensorflow 1.13.1 which is incompatible.\r\n",
      "pytorch-lightning 1.6.5 requires tensorboard>=2.2.0, but you have tensorboard 1.13.1 which is incompatible.\r\n",
      "explainable-ai-sdk 1.3.3 requires tensorflow>=1.15.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed astor-0.8.1 keras-applications-1.0.8 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541c93fe",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:08.872433Z",
     "iopub.status.busy": "2022-07-30T11:57:08.871591Z",
     "iopub.status.idle": "2022-07-30T11:57:09.914096Z",
     "shell.execute_reply": "2022-07-30T11:57:09.912952Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.05582,
     "end_time": "2022-07-30T11:57:09.916787",
     "exception": false,
     "start_time": "2022-07-30T11:57:08.860967",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843b0049",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:09.938501Z",
     "iopub.status.busy": "2022-07-30T11:57:09.938250Z",
     "iopub.status.idle": "2022-07-30T11:57:09.960289Z",
     "shell.execute_reply": "2022-07-30T11:57:09.959285Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.035425,
     "end_time": "2022-07-30T11:57:09.962593",
     "exception": false,
     "start_time": "2022-07-30T11:57:09.927168",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "    load parameters from pre-train models.\"\"\"\n",
    "\n",
    "    # 预训练好的模型的权重\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "\n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "\n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "\n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            # input, filter=[filter_height, filter_width, in_channels, out_channels], strides, padding\n",
    "            # [1, 1, 1, 1]各个维度的stride\n",
    "            conv = tf.nn.conv2d(x, conv_w, [1, 1, 1, 1], padding='SAME')\n",
    "            conv = tf.nn.bias_add(conv, conv_b)\n",
    "            conv = tf.nn.relu(conv)\n",
    "            return conv\n",
    "\n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1],\n",
    "                              padding='SAME',\n",
    "                              name=name)\n",
    "\n",
    "    def flattrn_layer(self, x, name):\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1  # 维度[image_width, image_height, channel]累乘 进行展平\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])  # -1会变为batch_size\n",
    "            return x\n",
    "\n",
    "    def fully_connected_layers(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            fc = tf.matmul(x, fc_w)\n",
    "            fc = tf.nn.bias_add(fc, fc_b)\n",
    "            if activation is None:\n",
    "                return fc\n",
    "            else:\n",
    "                return activation(fc)\n",
    "\n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"Build VGG16 network structure.\"\"\"\n",
    "        # x_rgb: [1, 224, 224, 3]\n",
    "        start_time = time.time()\n",
    "        print('building model...')\n",
    "        r, g, b = tf.split(x_rgb, [1, 1, 1], axis=3)\n",
    "        # 每个通道减去均值后再次合并\n",
    "        x_bgr = tf.concat([b - VGG_MEAN[0],\n",
    "                             g - VGG_MEAN[1],\n",
    "                             r - VGG_MEAN[2]], axis=3)\n",
    "        assert x_bgr.get_shape().as_list()[1:]==[224, 224, 3]\n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "\n",
    "        # 全连接层 耗时耗内存\n",
    "        \"\"\"self.flatten5 = self.flattrn_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fully_connected_layers(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fully_connected_layers(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fully_connected_layers(self.fc7, 'fc8')\n",
    "        self.prob = tf.nn.softmax(self.fc8, name='prob')\"\"\"\n",
    "\n",
    "        print('building model finished: %4ds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3087da2",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:09.990168Z",
     "iopub.status.busy": "2022-07-30T11:57:09.989794Z",
     "iopub.status.idle": "2022-07-30T11:57:16.491290Z",
     "shell.execute_reply": "2022-07-30T11:57:16.490277Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.521133,
     "end_time": "2022-07-30T11:57:16.493450",
     "exception": false,
     "start_time": "2022-07-30T11:57:09.972317",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model...\n",
      "building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "# 测试模型构建时间\n",
    "vgg16_npy_path = '../input/vgg16model/vgg16.npy'\n",
    "data_dict = np.load(vgg16_npy_path, encoding='latin1', allow_pickle=True).item()\n",
    "vgg16_for_result = VGGNet(data_dict)\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "vgg16_for_result.build(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853a2d6d",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:16.515182Z",
     "iopub.status.busy": "2022-07-30T11:57:16.514895Z",
     "iopub.status.idle": "2022-07-30T11:57:17.199227Z",
     "shell.execute_reply": "2022-07-30T11:57:17.197885Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.697258,
     "end_time": "2022-07-30T11:57:17.201506",
     "exception": false,
     "start_time": "2022-07-30T11:57:16.504248",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/run_ImageStyleTransfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69539f8",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:17.222669Z",
     "iopub.status.busy": "2022-07-30T11:57:17.222389Z",
     "iopub.status.idle": "2022-07-30T11:57:17.229989Z",
     "shell.execute_reply": "2022-07-30T11:57:17.229144Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02043,
     "end_time": "2022-07-30T11:57:17.231944",
     "exception": false,
     "start_time": "2022-07-30T11:57:17.211514",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg16_npy_path = '/kaggle/input/vgg16model/vgg16.npy'\n",
    "content_img_path = '/kaggle/input/imagestyletransfer/gugong.jpg'  # 内容图像路径\n",
    "style_img_path = '/kaggle/input/imagestyletransfer/xingkong.jpeg'  # 风格图像路径\n",
    "\n",
    "num_steps = 100  #训练步数\n",
    "learining_rate = 10\n",
    "lambda_content = 0.1  # 内容损失的系数 (内容损失一般较大)\n",
    "lambda_style = 500  # 风格损失系数 (风格损失一般较小)\n",
    "output_dir = '/kaggle/working/run_ImageStyleTransfer'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2534c12c",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:17.253077Z",
     "iopub.status.busy": "2022-07-30T11:57:17.252830Z",
     "iopub.status.idle": "2022-07-30T11:57:17.261093Z",
     "shell.execute_reply": "2022-07-30T11:57:17.260185Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021192,
     "end_time": "2022-07-30T11:57:17.263093",
     "exception": false,
     "start_time": "2022-07-30T11:57:17.241901",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 对图像进行初始化\n",
    "def initial_result(shape, mean, stddev):\n",
    "    # 截断产生均值为mean,标准差为stddev的正态分布的张量\n",
    "    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def read_image(image_name):\n",
    "    img = Image.open(image_name)\n",
    "    np_img = np.array(img)  # (224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32)  # (1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "\n",
    "# gram矩阵是计算每个通道i的feature map与每个通道j的feature map的内积\n",
    "# gram matrix的每个值可以说是代表i通道的feature map与j通道的feature map的互相关程度\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calulates gram matrix\"\"\"\n",
    "    # [1, width, height, ch]\n",
    "    # 获取各个维度的值，b是样本数，h高度，w宽度，ch通道数\n",
    "    b, h, w, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, h * w, ch])\n",
    "    # [h*w, ch]-->[ch, h*w] * [h*w, ch]-->[ch, ch]\n",
    "    # 计算任意两列的相似度 adjoint_a=True将第一个features转置\n",
    "    # 除以一个常量为防止最终的数值比较大\n",
    "    gram = tf.matmul(features, features, adjoint_a=True) / tf.constant(h * w * ch, tf.float32)\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e97756",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:17.283497Z",
     "iopub.status.busy": "2022-07-30T11:57:17.283252Z",
     "iopub.status.idle": "2022-07-30T11:57:18.816793Z",
     "shell.execute_reply": "2022-07-30T11:57:18.815775Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.54622,
     "end_time": "2022-07-30T11:57:18.818952",
     "exception": false,
     "start_time": "2022-07-30T11:57:17.272732",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model...\n",
      "building model finished:    0s\n",
      "building model...\n",
      "building model finished:    0s\n",
      "building model...\n",
      "building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "content_value = read_image(content_img_path)\n",
    "style_value = read_image(style_img_path)\n",
    "# 1.0版本需要\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "data_dict = np.load(vgg16_npy_path, encoding='latin1', allow_pickle=True).item()\n",
    "# 创建3个VGGNet\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "# 层次超参数 多层效果较好\n",
    "# 内容特征初始化层次\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    # vgg_for_content.conv2_2,\n",
    "    # vgg_for_content.conv3_3,\n",
    "    # vgg_for_content.conv4_3,\n",
    "    # vgg_for_content.conv5_3, \n",
    "]\n",
    "# 一定要和内容特征的层数保持一致\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    # vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3, \n",
    "]\n",
    "# 风格特征初始化层次\n",
    "style_features = [\n",
    "    # vgg_for_style.conv1_2,\n",
    "    # vgg_for_style.conv2_2,\n",
    "    # vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    # vgg_for_style.conv5_3, \n",
    "]\n",
    "# 给结果图像提取风格特征，和风格特征图像的层次必须一致\n",
    "result_style_features = [\n",
    "    # vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c1ac1ba",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:18.841165Z",
     "iopub.status.busy": "2022-07-30T11:57:18.840862Z",
     "iopub.status.idle": "2022-07-30T11:57:18.851908Z",
     "shell.execute_reply": "2022-07-30T11:57:18.851066Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023939,
     "end_time": "2022-07-30T11:57:18.853793",
     "exception": false,
     "start_time": "2022-07-30T11:57:18.829854",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 风格图像的gram矩阵，gram矩阵是两两通道之间的相似度\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "# 结果图像的gram矩阵\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfebaf49",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:18.875540Z",
     "iopub.status.busy": "2022-07-30T11:57:18.875073Z",
     "iopub.status.idle": "2022-07-30T11:57:19.249571Z",
     "shell.execute_reply": "2022-07-30T11:57:19.248675Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.387922,
     "end_time": "2022-07-30T11:57:19.252151",
     "exception": false,
     "start_time": "2022-07-30T11:57:18.864229",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# # shape: [1, width, height, channel]\n",
    "# 每一层计算损失\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "loss = content_loss * lambda_content + style_loss * lambda_style\n",
    "train_op = tf.train.AdamOptimizer(learining_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf1492c4",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-30T11:57:19.273508Z",
     "iopub.status.busy": "2022-07-30T11:57:19.273243Z",
     "iopub.status.idle": "2022-07-30T12:01:25.248524Z",
     "shell.execute_reply": "2022-07-30T12:01:25.246880Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 245.988311,
     "end_time": "2022-07-30T12:01:25.250629",
     "exception": false,
     "start_time": "2022-07-30T11:57:19.262318",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 11:57:19.279252: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2022-07-30 11:57:19.283735: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000194999 Hz\n",
      "2022-07-30 11:57:19.283977: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5640056a39f0 executing computations on platform Host. Devices:\n",
      "2022-07-30 11:57:19.284004: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 14285.2324, content_loss: 60667.9180, style_loss:  16.4369\n",
      "step: 2, loss_value: 11850.0840, content_loss: 46002.8164, style_loss:  14.4996\n",
      "step: 3, loss_value: 8979.9717, content_loss: 37214.1484, style_loss:  10.5171\n",
      "step: 4, loss_value: 7265.5693, content_loss: 32360.7480, style_loss:   8.0590\n",
      "step: 5, loss_value: 6518.0308, content_loss: 29499.8984, style_loss:   7.1361\n",
      "step: 6, loss_value: 6145.0835, content_loss: 27918.6113, style_loss:   6.7064\n",
      "step: 7, loss_value: 5186.3701, content_loss: 26989.9336, style_loss:   4.9748\n",
      "step: 8, loss_value: 5144.0249, content_loss: 26547.0156, style_loss:   4.9786\n",
      "step: 9, loss_value: 4565.4263, content_loss: 26261.2812, style_loss:   3.8786\n",
      "step: 10, loss_value: 4495.4644, content_loss: 26083.2070, style_loss:   3.7743\n",
      "step: 11, loss_value: 4245.8828, content_loss: 25916.8730, style_loss:   3.3084\n",
      "step: 12, loss_value: 4188.7603, content_loss: 25682.7246, style_loss:   3.2410\n",
      "step: 13, loss_value: 3941.8713, content_loss: 25298.9434, style_loss:   2.8240\n",
      "step: 14, loss_value: 3841.6021, content_loss: 24830.0723, style_loss:   2.7172\n",
      "step: 15, loss_value: 3676.0869, content_loss: 24278.1152, style_loss:   2.4966\n",
      "step: 16, loss_value: 3532.7578, content_loss: 23600.8770, style_loss:   2.3453\n",
      "step: 17, loss_value: 3405.8062, content_loss: 22823.8008, style_loss:   2.2469\n",
      "step: 18, loss_value: 3251.3674, content_loss: 22012.2031, style_loss:   2.1003\n",
      "step: 19, loss_value: 3127.5132, content_loss: 21166.8770, style_loss:   2.0217\n",
      "step: 20, loss_value: 2995.9438, content_loss: 20267.0762, style_loss:   1.9385\n",
      "step: 21, loss_value: 2856.6008, content_loss: 19365.0664, style_loss:   1.8402\n",
      "step: 22, loss_value: 2735.8040, content_loss: 18458.7051, style_loss:   1.7799\n",
      "step: 23, loss_value: 2630.8318, content_loss: 17569.8164, style_loss:   1.7477\n",
      "step: 24, loss_value: 2523.8218, content_loss: 16743.5996, style_loss:   1.6989\n",
      "step: 25, loss_value: 2431.0757, content_loss: 15958.9746, style_loss:   1.6704\n",
      "step: 26, loss_value: 2327.3892, content_loss: 15250.8779, style_loss:   1.6046\n",
      "step: 27, loss_value: 2257.0078, content_loss: 14567.6758, style_loss:   1.6005\n",
      "step: 28, loss_value: 2171.6653, content_loss: 13960.9697, style_loss:   1.5511\n",
      "step: 29, loss_value: 2120.1882, content_loss: 13383.5371, style_loss:   1.5637\n",
      "step: 30, loss_value: 2020.7791, content_loss: 12884.8086, style_loss:   1.4646\n",
      "step: 31, loss_value: 1945.5852, content_loss: 12405.8877, style_loss:   1.4100\n",
      "step: 32, loss_value: 1875.0145, content_loss: 11980.8623, style_loss:   1.3539\n",
      "step: 33, loss_value: 1818.8221, content_loss: 11586.8574, style_loss:   1.3203\n",
      "step: 34, loss_value: 1777.5513, content_loss: 11212.7363, style_loss:   1.3126\n",
      "step: 35, loss_value: 1771.5774, content_loss: 10880.7344, style_loss:   1.3670\n",
      "step: 36, loss_value: 1926.2212, content_loss: 10551.8906, style_loss:   1.7421\n",
      "step: 37, loss_value: 1806.2910, content_loss: 10336.1836, style_loss:   1.5453\n",
      "step: 38, loss_value: 1715.5679, content_loss: 10118.1514, style_loss:   1.4075\n",
      "step: 39, loss_value: 1643.0287, content_loss: 9941.7969, style_loss:   1.2977\n",
      "step: 40, loss_value: 1687.4221, content_loss: 9813.1221, style_loss:   1.4122\n",
      "step: 41, loss_value: 1607.3165, content_loss: 9666.3574, style_loss:   1.2814\n",
      "step: 42, loss_value: 1560.9775, content_loss: 9541.4297, style_loss:   1.2137\n",
      "step: 43, loss_value: 1556.5532, content_loss: 9408.3223, style_loss:   1.2314\n",
      "step: 44, loss_value: 1521.7826, content_loss: 9243.6338, style_loss:   1.1948\n",
      "step: 45, loss_value: 1480.2556, content_loss: 9113.8164, style_loss:   1.1377\n",
      "step: 46, loss_value: 1442.9653, content_loss: 8965.8486, style_loss:   1.0928\n",
      "step: 47, loss_value: 1416.4410, content_loss: 8812.7725, style_loss:   1.0703\n",
      "step: 48, loss_value: 1396.0737, content_loss: 8663.3135, style_loss:   1.0595\n",
      "step: 49, loss_value: 1371.9473, content_loss: 8494.8213, style_loss:   1.0449\n",
      "step: 50, loss_value: 1376.3966, content_loss: 8343.2168, style_loss:   1.0841\n",
      "step: 51, loss_value: 1468.8511, content_loss: 8172.9629, style_loss:   1.3031\n",
      "step: 52, loss_value: 1514.3037, content_loss: 8071.8457, style_loss:   1.4142\n",
      "step: 53, loss_value: 1593.9772, content_loss: 7933.6069, style_loss:   1.6012\n",
      "step: 54, loss_value: 1364.9009, content_loss: 7897.9648, style_loss:   1.1502\n",
      "step: 55, loss_value: 1481.0660, content_loss: 7907.8418, style_loss:   1.3806\n",
      "step: 56, loss_value: 1404.1034, content_loss: 7882.3330, style_loss:   1.2317\n",
      "step: 57, loss_value: 1362.5146, content_loss: 7887.4785, style_loss:   1.1475\n",
      "step: 58, loss_value: 1368.1664, content_loss: 7909.2627, style_loss:   1.1545\n",
      "step: 59, loss_value: 1350.1409, content_loss: 7898.1543, style_loss:   1.1207\n",
      "step: 60, loss_value: 1314.4573, content_loss: 7879.0591, style_loss:   1.0531\n",
      "step: 61, loss_value: 1321.9434, content_loss: 7854.2598, style_loss:   1.0730\n",
      "step: 62, loss_value: 1280.4847, content_loss: 7782.9834, style_loss:   1.0044\n",
      "step: 63, loss_value: 1265.0396, content_loss: 7705.3730, style_loss:   0.9890\n",
      "step: 64, loss_value: 1241.1820, content_loss: 7613.8965, style_loss:   0.9596\n",
      "step: 65, loss_value: 1226.3551, content_loss: 7517.9434, style_loss:   0.9491\n",
      "step: 66, loss_value: 1210.4462, content_loss: 7422.9272, style_loss:   0.9363\n",
      "step: 67, loss_value: 1193.1987, content_loss: 7302.0435, style_loss:   0.9260\n",
      "step: 68, loss_value: 1175.9506, content_loss: 7189.9517, style_loss:   0.9139\n",
      "step: 69, loss_value: 1162.8191, content_loss: 7062.3765, style_loss:   0.9132\n",
      "step: 70, loss_value: 1152.3495, content_loss: 6953.8203, style_loss:   0.9139\n",
      "step: 71, loss_value: 1167.9915, content_loss: 6823.4258, style_loss:   0.9713\n",
      "step: 72, loss_value: 1187.8184, content_loss: 6743.3945, style_loss:   1.0270\n",
      "step: 73, loss_value: 1255.9709, content_loss: 6625.5381, style_loss:   1.1868\n",
      "step: 74, loss_value: 1136.0325, content_loss: 6591.9248, style_loss:   0.9537\n",
      "step: 75, loss_value: 1125.2606, content_loss: 6546.5996, style_loss:   0.9412\n",
      "step: 76, loss_value: 1158.3948, content_loss: 6487.9414, style_loss:   1.0192\n",
      "step: 77, loss_value: 1117.4049, content_loss: 6471.6606, style_loss:   0.9405\n",
      "step: 78, loss_value: 1087.2729, content_loss: 6438.6250, style_loss:   0.8868\n",
      "step: 79, loss_value: 1098.1566, content_loss: 6402.4258, style_loss:   0.9158\n",
      "step: 80, loss_value: 1080.5723, content_loss: 6379.5444, style_loss:   0.8852\n",
      "step: 81, loss_value: 1086.2574, content_loss: 6317.9976, style_loss:   0.9089\n",
      "step: 82, loss_value: 1065.3452, content_loss: 6263.9561, style_loss:   0.8779\n",
      "step: 83, loss_value: 1072.0078, content_loss: 6186.3726, style_loss:   0.9067\n",
      "step: 84, loss_value: 1058.5048, content_loss: 6129.9077, style_loss:   0.8910\n",
      "step: 85, loss_value: 1047.6488, content_loss: 6069.4351, style_loss:   0.8814\n",
      "step: 86, loss_value: 1041.0186, content_loss: 6036.6045, style_loss:   0.8747\n",
      "step: 87, loss_value: 1066.9524, content_loss: 5962.0444, style_loss:   0.9415\n",
      "step: 88, loss_value: 1060.7902, content_loss: 5934.5620, style_loss:   0.9347\n",
      "step: 89, loss_value: 1086.3090, content_loss: 5872.9243, style_loss:   0.9980\n",
      "step: 90, loss_value: 1030.4230, content_loss: 5873.5171, style_loss:   0.8861\n",
      "step: 91, loss_value: 1034.4098, content_loss: 5849.6851, style_loss:   0.8989\n",
      "step: 92, loss_value: 1022.0984, content_loss: 5830.3633, style_loss:   0.8781\n",
      "step: 93, loss_value: 1023.0568, content_loss: 5821.5308, style_loss:   0.8818\n",
      "step: 94, loss_value: 1003.0948, content_loss: 5785.9121, style_loss:   0.8490\n",
      "step: 95, loss_value: 987.8264, content_loss: 5750.9912, style_loss:   0.8255\n",
      "step: 96, loss_value: 987.7822, content_loss: 5731.1943, style_loss:   0.8293\n",
      "step: 97, loss_value: 974.8113, content_loss: 5683.8662, style_loss:   0.8128\n",
      "step: 98, loss_value: 980.9748, content_loss: 5644.3994, style_loss:   0.8331\n",
      "step: 99, loss_value: 1025.9510, content_loss: 5589.1465, style_loss:   0.9341\n",
      "step: 100, loss_value: 1130.8872, content_loss: 5587.9121, style_loss:   1.1442\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as se:\n",
    "    se.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = se.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict={content: content_value, style: style_value})\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f'\n",
    "              % (step + 1, loss_value[0], content_loss_value[0], style_loss_value[0]))\n",
    "        # 存储每一步的结果图像\n",
    "        result_img_path = os.path.join(output_dir, 'result_%05d.jpg' % (step + 1))\n",
    "        result_value=result.eval(se)[0]\n",
    "        result_value=np.clip(result_value,0,255)\n",
    "        img_arr=np.asarray(result_value,np.uint8)\n",
    "        img=Image.fromarray(img_arr)\n",
    "        img.save(result_img_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 310.607717,
   "end_time": "2022-07-30T12:01:25.988191",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-30T11:56:15.380474",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
