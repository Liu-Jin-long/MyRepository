{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627, 10)\n",
      "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
      "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
      "1         1  female  38.0                   1      0  71.2833  First        C   \n",
      "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
      "3         1  female  35.0                   1      0  53.1000  First        C   \n",
      "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
      "\n",
      "   embark_town alone  \n",
      "0  Southampton     n  \n",
      "1    Cherbourg     n  \n",
      "2  Southampton     y  \n",
      "3  Southampton     n  \n",
      "4   Queenstown     y  \n",
      "   survived     sex   age  n_siblings_spouses  parch     fare   class  \\\n",
      "0         0    male  35.0                   0      0   8.0500   Third   \n",
      "1         0    male  54.0                   0      0  51.8625   First   \n",
      "2         1  female  58.0                   0      0  26.5500   First   \n",
      "3         1  female  55.0                   0      0  16.0000  Second   \n",
      "4         1    male  34.0                   0      0  13.0000  Second   \n",
      "\n",
      "      deck  embark_town alone  \n",
      "0  unknown  Southampton     y  \n",
      "1        E  Southampton     y  \n",
      "2        C  Southampton     y  \n",
      "3  unknown  Southampton     y  \n",
      "4        D  Southampton     y  \n"
     ]
    }
   ],
   "source": [
    "# https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
    "# https://storage.googleapis.com/tf-datasets/titanic/eval.csv\n",
    "train_file = \"./data/titanic/train.csv\"\n",
    "eval_file = \"./data/titanic/eval.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "eval_df = pd.read_csv(eval_file)\n",
    "print(train_df.shape)\n",
    "print(train_df.head())\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
      "0    male  22.0                   1      0   7.2500  Third  unknown   \n",
      "1  female  38.0                   1      0  71.2833  First        C   \n",
      "2  female  26.0                   0      0   7.9250  Third  unknown   \n",
      "3  female  35.0                   1      0  53.1000  First        C   \n",
      "4    male  28.0                   0      0   8.4583  Third  unknown   \n",
      "\n",
      "   embark_town alone  \n",
      "0  Southampton     n  \n",
      "1    Cherbourg     n  \n",
      "2  Southampton     y  \n",
      "3  Southampton     n  \n",
      "4   Queenstown     y  \n",
      "      sex   age  n_siblings_spouses  parch     fare   class     deck  \\\n",
      "0    male  35.0                   0      0   8.0500   Third  unknown   \n",
      "1    male  54.0                   0      0  51.8625   First        E   \n",
      "2  female  58.0                   0      0  26.5500   First        C   \n",
      "3  female  55.0                   0      0  16.0000  Second  unknown   \n",
      "4    male  34.0                   0      0  13.0000  Second        D   \n",
      "\n",
      "   embark_town alone  \n",
      "0  Southampton     y  \n",
      "1  Southampton     y  \n",
      "2  Southampton     y  \n",
      "3  Southampton     y  \n",
      "4  Southampton     y  \n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: survived, dtype: int64\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_train = train_df.pop('survived')\n",
    "y_eval = eval_df.pop('survived')\n",
    "\n",
    "print(train_df.head())\n",
    "print(eval_df.head())\n",
    "print(y_train.head())\n",
    "print(y_eval.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              age  n_siblings_spouses       parch        fare\n",
      "count  627.000000          627.000000  627.000000  627.000000\n",
      "mean    29.631308            0.545455    0.379585   34.385399\n",
      "std     12.511818            1.151090    0.792999   54.597730\n",
      "min      0.750000            0.000000    0.000000    0.000000\n",
      "25%     23.000000            0.000000    0.000000    7.895800\n",
      "50%     28.000000            0.000000    0.000000   15.045800\n",
      "75%     35.000000            1.000000    0.000000   31.387500\n",
      "max     80.000000            8.000000    5.000000  512.329200\n",
      "         sex  class     deck  embark_town alone\n",
      "count    627    627      627          627   627\n",
      "unique     2      3        8            4     2\n",
      "top     male  Third  unknown  Southampton     y\n",
      "freq     410    341      481          450   372\n"
     ]
    }
   ],
   "source": [
    "print(train_df.describe())\n",
    "print(train_df.describe(include=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex ['male' 'female']\n",
      "n_siblings_spouses [1 0 3 4 2 5 8]\n",
      "parch [0 1 2 5 3 4]\n",
      "class ['Third' 'First' 'Second']\n",
      "deck ['unknown' 'C' 'G' 'A' 'B' 'D' 'F' 'E']\n",
      "embark_town ['Southampton' 'Cherbourg' 'Queenstown' 'unknown']\n",
      "alone ['n' 'y']\n",
      "age\n",
      "fare\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['sex', 'n_siblings_spouses', 'parch', 'class',\n",
    "                       'deck', 'embark_town', 'alone']\n",
    "numeric_columns = ['age', 'fare']\n",
    "\n",
    "feature_columns = []\n",
    "for categorical_column in categorical_columns:\n",
    "    vocab = train_df[categorical_column].unique()\n",
    "    print(categorical_column, vocab)\n",
    "    feature_columns.append(\n",
    "        tf.feature_column.indicator_column(\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "                categorical_column, vocab)))\n",
    "\n",
    "for categorical_column in numeric_columns:\n",
    "    print(categorical_column)\n",
    "    feature_columns.append(\n",
    "        tf.feature_column.numeric_column(\n",
    "            categorical_column, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='n_siblings_spouses', vocabulary_list=(1, 0, 3, 4, 2, 5, 8), dtype=tf.int64, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='parch', vocabulary_list=(0, 1, 2, 5, 3, 4), dtype=tf.int64, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='class', vocabulary_list=('Third', 'First', 'Second'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='deck', vocabulary_list=('unknown', 'C', 'G', 'A', 'B', 'D', 'F', 'E'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Southampton', 'Cherbourg', 'Queenstown', 'unknown'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='alone', vocabulary_list=('n', 'y'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='fare', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    }
   ],
   "source": [
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(627, 9)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_df))\n",
    "print(train_df.shape)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'sex': <tf.Tensor: shape=(), dtype=string, numpy=b'male'>, 'age': <tf.Tensor: shape=(), dtype=float64, numpy=22.0>, 'n_siblings_spouses': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'parch': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'fare': <tf.Tensor: shape=(), dtype=float64, numpy=7.25>, 'class': <tf.Tensor: shape=(), dtype=string, numpy=b'Third'>, 'deck': <tf.Tensor: shape=(), dtype=string, numpy=b'unknown'>, 'embark_town': <tf.Tensor: shape=(), dtype=string, numpy=b'Southampton'>, 'alone': <tf.Tensor: shape=(), dtype=string, numpy=b'n'>}, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# 注意传入的类型不同，生成效果不同\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(train_df), y_train))\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_dataset(data_df, label_df, epochs=10, shuffle=True,\n",
    "                 batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dict(data_df), label_df))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    # 必须是repeat类型的dataset，进行分批\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'linear_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From D:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From D:\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\ftrl.py:153: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from linear_model\\model.ckpt-1960\n",
      "WARNING:tensorflow:From D:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1175: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1960...\n",
      "INFO:tensorflow:Saving checkpoints for 1960 into linear_model\\model.ckpt.\n",
      "INFO:tensorflow:linear_model\\model.ckpt-1960.data-00000-of-00001\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:linear_model\\model.ckpt-1960.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:linear_model\\model.ckpt-1960.meta\n",
      "INFO:tensorflow:400\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1960...\n",
      "INFO:tensorflow:loss = 0.39495063, step = 1960\n",
      "INFO:tensorflow:global_step/sec: 531.897\n",
      "INFO:tensorflow:loss = 0.54592526, step = 2060 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 1012.8\n",
      "INFO:tensorflow:loss = 0.30863875, step = 2160 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.69\n",
      "INFO:tensorflow:loss = 0.5859081, step = 2260 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1023.13\n",
      "INFO:tensorflow:loss = 0.4305965, step = 2360 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 964.11\n",
      "INFO:tensorflow:loss = 0.42791057, step = 2460 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 1033.68\n",
      "INFO:tensorflow:loss = 0.41871864, step = 2560 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1022.76\n",
      "INFO:tensorflow:loss = 0.36265042, step = 2660 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.66\n",
      "INFO:tensorflow:loss = 0.27183887, step = 2760 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1165.91\n",
      "INFO:tensorflow:loss = 0.5310483, step = 2860 (0.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 1023.12\n",
      "INFO:tensorflow:loss = 0.25157255, step = 2960 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1057.04\n",
      "INFO:tensorflow:loss = 0.6317133, step = 3060 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1012.59\n",
      "INFO:tensorflow:loss = 0.3202529, step = 3160 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 1012.31\n",
      "INFO:tensorflow:loss = 0.40101886, step = 3260 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 1023.13\n",
      "INFO:tensorflow:loss = 0.3099445, step = 3360 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1003.95\n",
      "INFO:tensorflow:loss = 0.36842018, step = 3460 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 970.01\n",
      "INFO:tensorflow:loss = 0.5260969, step = 3560 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 1023.14\n",
      "INFO:tensorflow:loss = 0.38280353, step = 3660 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1055.44\n",
      "INFO:tensorflow:loss = 0.47538453, step = 3760 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1033.16\n",
      "INFO:tensorflow:loss = 0.40234452, step = 3860 (0.096 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3920...\n",
      "INFO:tensorflow:Saving checkpoints for 3920 into linear_model\\model.ckpt.\n",
      "INFO:tensorflow:linear_model\\model.ckpt-3920.data-00000-of-00001\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:linear_model\\model.ckpt-3920.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:linear_model\\model.ckpt-3920.meta\n",
      "INFO:tensorflow:400\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3920...\n",
      "INFO:tensorflow:Loss for final step: 0.39851138.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearClassifierV2 at 0x28f7cfa16a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output_dir = 'linear_model'\n",
    "if not os.path.exists(linear_output_dir):\n",
    "    os.mkdir(linear_output_dir)\n",
    "# 线性分类器模型\n",
    "linear_estimator = tf.estimator.LinearClassifier(model_dir=linear_output_dir, n_classes=2,\n",
    "                                                 feature_columns=feature_columns)\n",
    "linear_estimator.train(input_fn=lambda: make_dataset(train_df, y_train, epochs=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['global_step', 'linear/linear_model/age/weights', 'linear/linear_model/alone_indicator/weights', 'linear/linear_model/bias_weights', 'linear/linear_model/class_indicator/weights', 'linear/linear_model/deck_indicator/weights', 'linear/linear_model/embark_town_indicator/weights', 'linear/linear_model/fare/weights', 'linear/linear_model/n_siblings_spouses_indicator/weights', 'linear/linear_model/parch_indicator/weights', 'linear/linear_model/sex_indicator/weights', 'training/Ftrl/beta', 'training/Ftrl/decay', 'training/Ftrl/l1_regularization_strength', 'training/Ftrl/l2_regularization_strength', 'training/Ftrl/learning_rate', 'training/Ftrl/learning_rate_power', 'training/Ftrl/linear/linear_model/age/weights/accumulator', 'training/Ftrl/linear/linear_model/age/weights/linear', 'training/Ftrl/linear/linear_model/alone_indicator/weights/accumulator', 'training/Ftrl/linear/linear_model/alone_indicator/weights/linear', 'training/Ftrl/linear/linear_model/bias_weights/accumulator', 'training/Ftrl/linear/linear_model/bias_weights/linear', 'training/Ftrl/linear/linear_model/class_indicator/weights/accumulator', 'training/Ftrl/linear/linear_model/class_indicator/weights/linear', 'training/Ftrl/linear/linear_model/deck_indicator/weights/accumulator', 'training/Ftrl/linear/linear_model/deck_indicator/weights/linear', 'training/Ftrl/linear/linear_model/embark_town_indicator/weights/accumulator', 'training/Ftrl/linear/linear_model/embark_town_indicator/weights/linear', 'training/Ftrl/linear/linear_model/fare/weights/accumulator', 'training/Ftrl/linear/linear_model/fare/weights/linear', 'training/Ftrl/linear/linear_model/n_siblings_spouses_indicator/weights/accumulator', 'training/Ftrl/linear/linear_model/n_siblings_spouses_indicator/weights/linear', 'training/Ftrl/linear/linear_model/parch_indicator/weights/accumulator', 'training/Ftrl/linear/linear_model/parch_indicator/weights/linear', 'training/Ftrl/linear/linear_model/sex_indicator/weights/accumulator', 'training/Ftrl/linear/linear_model/sex_indicator/weights/linear']\n"
     ]
    }
   ],
   "source": [
    "print(linear_estimator.get_variable_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7746453]\n",
      " [-4.321594 ]\n",
      " [-2.8215318]\n",
      " [ 1.7765334]\n",
      " [-1.3340805]\n",
      " [ 3.1115713]]\n",
      "[[15.951467  ]\n",
      " [ 2.3644955 ]\n",
      " [ 1.8324596 ]\n",
      " [ 0.22464252]\n",
      " [ 0.12601449]\n",
      " [ 0.13502812]]\n"
     ]
    }
   ],
   "source": [
    "print(linear_estimator.get_variable_value('training/Ftrl/linear/linear_model/parch_indicator/weights/linear'))\n",
    "print(linear_estimator.get_variable_value('training/Ftrl/linear/linear_model/parch_indicator/weights/accumulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './dnn_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./dnn_model\\model.ckpt.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 142: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Input \u001B[1;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m dnn_estimator \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mestimator\u001B[38;5;241m.\u001B[39mDNNClassifier(\n\u001B[0;32m      6\u001B[0m     model_dir \u001B[38;5;241m=\u001B[39m dnn_output_dir,\n\u001B[0;32m      7\u001B[0m     n_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m     activation_fn \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mrelu,\n\u001B[0;32m     11\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdam\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m#开始训练\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[43mdnn_estimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_fn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmake_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:360\u001B[0m, in \u001B[0;36mEstimator.train\u001B[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001B[0m\n\u001B[0;32m    357\u001B[0m hooks\u001B[38;5;241m.\u001B[39mextend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_train_steps_to_hooks(steps, max_steps))\n\u001B[0;32m    359\u001B[0m saving_listeners \u001B[38;5;241m=\u001B[39m _check_listeners_type(saving_listeners)\n\u001B[1;32m--> 360\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msaving_listeners\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    361\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoss for final step: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m, loss)\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:1186\u001B[0m, in \u001B[0;36mEstimator._train_model\u001B[1;34m(self, input_fn, hooks, saving_listeners)\u001B[0m\n\u001B[0;32m   1184\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_model_distributed(input_fn, hooks, saving_listeners)\n\u001B[0;32m   1185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1186\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_model_default\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msaving_listeners\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:1217\u001B[0m, in \u001B[0;36mEstimator._train_model_default\u001B[1;34m(self, input_fn, hooks, saving_listeners)\u001B[0m\n\u001B[0;32m   1214\u001B[0m estimator_spec \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_model_fn(features, labels, ModeKeys\u001B[38;5;241m.\u001B[39mTRAIN,\n\u001B[0;32m   1215\u001B[0m                                      \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig)\n\u001B[0;32m   1216\u001B[0m global_step_tensor \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39mv1\u001B[38;5;241m.\u001B[39mtrain\u001B[38;5;241m.\u001B[39mget_global_step(g)\n\u001B[1;32m-> 1217\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_with_estimator_spec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator_spec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworker_hooks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1218\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglobal_step_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1219\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43msaving_listeners\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:1512\u001B[0m, in \u001B[0;36mEstimator._train_with_estimator_spec\u001B[1;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001B[0m\n\u001B[0;32m   1505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mlog_step_count_steps \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mlog_step_count_steps \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m   1507\u001B[0m       worker_hooks\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m   1508\u001B[0m           tf\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39mv1\u001B[38;5;241m.\u001B[39mtrain\u001B[38;5;241m.\u001B[39mStepCounterHook(\n\u001B[0;32m   1509\u001B[0m               every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mlog_step_count_steps,\n\u001B[0;32m   1510\u001B[0m               output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mmodel_dir))\n\u001B[1;32m-> 1512\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mtraining\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMonitoredTrainingSession\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1513\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaster\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaster\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1514\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_chief\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_chief\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1515\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1516\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscaffold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_spec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaffold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1517\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworker_hooks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1518\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchief_only_hooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mchief_hooks\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[0;32m   1519\u001B[0m \u001B[43m                      \u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mestimator_spec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_chief_hooks\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1520\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_checkpoint_secs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Saving is handled by a hook.\u001B[39;49;00m\n\u001B[0;32m   1521\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_summaries_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_summary_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1523\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_wait_secs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msession_creation_timeout_secs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlog_step_count_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_step_count_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_graph_def\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheckpoint_save_graph_def\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m mon_sess:\n\u001B[0;32m   1526\u001B[0m   loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1527\u001B[0m   current_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:609\u001B[0m, in \u001B[0;36mMonitoredTrainingSession\u001B[1;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir, save_graph_def)\u001B[0m\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hooks:\n\u001B[0;32m    608\u001B[0m   all_hooks\u001B[38;5;241m.\u001B[39mextend(hooks)\n\u001B[1;32m--> 609\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mMonitoredSession\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    610\u001B[0m \u001B[43m    \u001B[49m\u001B[43msession_creator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession_creator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    611\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_hooks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstop_grace_period_secs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop_grace_period_secs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:1054\u001B[0m, in \u001B[0;36mMonitoredSession.__init__\u001B[1;34m(self, session_creator, hooks, stop_grace_period_secs)\u001B[0m\n\u001B[0;32m   1050\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1051\u001B[0m              session_creator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1052\u001B[0m              hooks\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1053\u001B[0m              stop_grace_period_secs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m120\u001B[39m):\n\u001B[1;32m-> 1054\u001B[0m   \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mMonitoredSession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1055\u001B[0m \u001B[43m      \u001B[49m\u001B[43msession_creator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1056\u001B[0m \u001B[43m      \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1057\u001B[0m \u001B[43m      \u001B[49m\u001B[43mshould_recover\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1058\u001B[0m \u001B[43m      \u001B[49m\u001B[43mstop_grace_period_secs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop_grace_period_secs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:757\u001B[0m, in \u001B[0;36m_MonitoredSession.__init__\u001B[1;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001B[0m\n\u001B[0;32m    752\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_coordinated_creator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_CoordinatedSessionCreator(\n\u001B[0;32m    753\u001B[0m     session_creator\u001B[38;5;241m=\u001B[39msession_creator \u001B[38;5;129;01mor\u001B[39;00m ChiefSessionCreator(),\n\u001B[0;32m    754\u001B[0m     hooks\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hooks,\n\u001B[0;32m    755\u001B[0m     stop_grace_period_secs\u001B[38;5;241m=\u001B[39mstop_grace_period_secs)\n\u001B[0;32m    756\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_recover:\n\u001B[1;32m--> 757\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sess \u001B[38;5;241m=\u001B[39m \u001B[43m_RecoverableSession\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_coordinated_creator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    758\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    759\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sess \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_coordinated_creator\u001B[38;5;241m.\u001B[39mcreate_session()\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:1263\u001B[0m, in \u001B[0;36m_RecoverableSession.__init__\u001B[1;34m(self, sess_creator)\u001B[0m\n\u001B[0;32m   1254\u001B[0m \u001B[38;5;124;03m\"\"\"Create a new `_RecoverableSession`.\u001B[39;00m\n\u001B[0;32m   1255\u001B[0m \n\u001B[0;32m   1256\u001B[0m \u001B[38;5;124;03mThe value returned by calling `sess_creator.create_session()` will be the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1260\u001B[0m \u001B[38;5;124;03m  sess_creator: A 'SessionCreator' to be wrapped by recoverable.\u001B[39;00m\n\u001B[0;32m   1261\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1262\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sess_creator \u001B[38;5;241m=\u001B[39m sess_creator\n\u001B[1;32m-> 1263\u001B[0m _WrappedSession\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:1268\u001B[0m, in \u001B[0;36m_RecoverableSession._create_session\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1266\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m   1267\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sess_creator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1269\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _PREEMPTION_ERRORS \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1270\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   1271\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAn error was raised while a session was being created. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1272\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThis may be due to a preemption of a connected worker \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1277\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mincreasing the number of parameter servers assigned to \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1278\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthe job. Error: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, e)\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:917\u001B[0m, in \u001B[0;36m_MonitoredSession._CoordinatedSessionCreator.create_session\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    915\u001B[0m \u001B[38;5;66;03m# Inform the hooks that a new session has been created.\u001B[39;00m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hooks:\n\u001B[1;32m--> 917\u001B[0m   \u001B[43mhook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mafter_create_session\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtf_sess\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoord\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _CoordinatedSession(\n\u001B[0;32m    919\u001B[0m     _HookedSession(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtf_sess, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hooks), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoord,\n\u001B[0;32m    920\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop_grace_period_secs)\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py:602\u001B[0m, in \u001B[0;36mCheckpointSaverHook.after_create_session\u001B[1;34m(self, session, coord)\u001B[0m\n\u001B[0;32m    600\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_summary_writer\u001B[38;5;241m.\u001B[39madd_meta_graph(meta_graph_def)\n\u001B[0;32m    601\u001B[0m \u001B[38;5;66;03m# The checkpoint saved here is the state at step \"global_step\".\u001B[39;00m\n\u001B[1;32m--> 602\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglobal_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    603\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timer\u001B[38;5;241m.\u001B[39mupdate_last_triggered_step(global_step)\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py:634\u001B[0m, in \u001B[0;36mCheckpointSaverHook._save\u001B[1;34m(self, session, step)\u001B[0m\n\u001B[0;32m    631\u001B[0m   l\u001B[38;5;241m.\u001B[39mbefore_save(session, step)\n\u001B[0;32m    633\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving checkpoints for \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m into \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, step, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_path)\n\u001B[1;32m--> 634\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_saver\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglobal_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mwrite_meta_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_graph_def\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_summary_writer\u001B[38;5;241m.\u001B[39madd_session_log(\n\u001B[0;32m    637\u001B[0m     SessionLog(\n\u001B[0;32m    638\u001B[0m         status\u001B[38;5;241m=\u001B[39mSessionLog\u001B[38;5;241m.\u001B[39mCHECKPOINT, checkpoint_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_path),\n\u001B[0;32m    639\u001B[0m     step)\n\u001B[0;32m    640\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCalling checkpoint listeners after saving checkpoint \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    641\u001B[0m              step)\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1286\u001B[0m, in \u001B[0;36mSaver.save\u001B[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001B[0m\n\u001B[0;32m   1284\u001B[0m   model_checkpoint_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msaver_def\u001B[38;5;241m.\u001B[39msave_tensor_name\n\u001B[0;32m   1285\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1286\u001B[0m   model_checkpoint_path \u001B[38;5;241m=\u001B[39m \u001B[43msess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1287\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaver_def\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_tensor_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1288\u001B[0m \u001B[43m      \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaver_def\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilename_tensor_name\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheckpoint_file\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1290\u001B[0m model_checkpoint_path \u001B[38;5;241m=\u001B[39m compat\u001B[38;5;241m.\u001B[39mas_str(model_checkpoint_path)\n\u001B[0;32m   1291\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m write_state:\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:967\u001B[0m, in \u001B[0;36mBaseSession.run\u001B[1;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m    964\u001B[0m run_metadata_ptr \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_NewBuffer() \u001B[38;5;28;01mif\u001B[39;00m run_metadata \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    966\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 967\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions_ptr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mrun_metadata_ptr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    969\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m run_metadata:\n\u001B[0;32m    970\u001B[0m     proto_data \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1190\u001B[0m, in \u001B[0;36mBaseSession._run\u001B[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001B[39;00m\n\u001B[0;32m   1189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m final_fetches \u001B[38;5;129;01mor\u001B[39;00m final_targets \u001B[38;5;129;01mor\u001B[39;00m (handle \u001B[38;5;129;01mand\u001B[39;00m feed_dict_tensor):\n\u001B[1;32m-> 1190\u001B[0m   results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_targets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_fetches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mfeed_dict_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1193\u001B[0m   results \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1370\u001B[0m, in \u001B[0;36mBaseSession._do_run\u001B[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1367\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001B[0;32m   1369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1370\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_run_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1371\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1372\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1373\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1377\u001B[0m, in \u001B[0;36mBaseSession._do_call\u001B[1;34m(self, fn, *args)\u001B[0m\n\u001B[0;32m   1375\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_do_call\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m   1376\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1377\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1378\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mOpError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1379\u001B[0m     message \u001B[38;5;241m=\u001B[39m compat\u001B[38;5;241m.\u001B[39mas_text(e\u001B[38;5;241m.\u001B[39mmessage)\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1360\u001B[0m, in \u001B[0;36mBaseSession._do_run.<locals>._run_fn\u001B[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[0;32m   1357\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_fn\u001B[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001B[0;32m   1358\u001B[0m   \u001B[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001B[39;00m\n\u001B[0;32m   1359\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extend_graph()\n\u001B[1;32m-> 1360\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_tf_sessionrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetch_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1361\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mtarget_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1453\u001B[0m, in \u001B[0;36mBaseSession._call_tf_sessionrun\u001B[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[0;32m   1451\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_tf_sessionrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, options, feed_dict, fetch_list, target_list,\n\u001B[0;32m   1452\u001B[0m                         run_metadata):\n\u001B[1;32m-> 1453\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_SessionRun_wrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1454\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mfetch_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1455\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xd5 in position 142: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "dnn_output_dir = './dnn_model'\n",
    "if not os.path.exists(dnn_output_dir):\n",
    "    os.mkdir(dnn_output_dir)\n",
    "# dnn估计器\n",
    "dnn_estimator = tf.estimator.DNNClassifier(\n",
    "    model_dir = dnn_output_dir,\n",
    "    n_classes = 2,\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units = [128, 128,128],\n",
    "    activation_fn = tf.nn.relu,\n",
    "    optimizer = 'Adam')\n",
    "#开始训练\n",
    "\n",
    "dnn_estimator.train(input_fn = lambda : make_dataset(\n",
    "    train_df, y_train, epochs = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(dnn_estimator.get_variable_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(dnn_estimator.get_variable_value('training/Adam/dnn/hiddenlayer_0/kernel/m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dnn_estimator.evaluate(input_fn=lambda: make_dataset(\n",
    "    eval_df, y_eval, epochs=1, shuffle=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}